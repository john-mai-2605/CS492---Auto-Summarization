{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Seq2seq+attention+coverage+embedding+beam.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "z6YcmNYRS2XN",
        "colab_type": "code",
        "outputId": "7495c81b-35bd-415c-ba54-bcb1b651268a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "try:\n",
        "    %tensorflow_version 2.x\n",
        "except Exception:\n",
        "    pass\n",
        "\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import os"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TensorFlow 2.x selected.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qq85hhsbhYug",
        "colab_type": "code",
        "outputId": "42eeb629-ac6a-40cc-c7b5-5cfeca85f8c6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "from google.colab import files\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8XFPMTnOV7rV",
        "colab_type": "text"
      },
      "source": [
        "Load the text files"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7iSTmwbxS7cJ",
        "colab_type": "code",
        "outputId": "03184da2-ee9a-47b7-8450-6c3c2ee3f227",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 384
        }
      },
      "source": [
        "if not os.path.exists('tok.valid.abstract.txt'):\n",
        "  !wget -O 'tok.valid.abstract.txt' 'https://www.dropbox.com/s/0c6e9yf8yhf9a75/tok.valid.abstract.txt?dl=1'\n",
        "if not os.path.exists('tok.valid.body.txt'):\n",
        "  !wget -O 'tok.valid.title.txt' 'https://www.dropbox.com/s/aiy87847kusb7ju/tok.valid.title.txt?dl=1'"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2019-12-02 17:15:40--  https://www.dropbox.com/s/aiy87847kusb7ju/tok.valid.title.txt?dl=1\n",
            "Resolving www.dropbox.com (www.dropbox.com)... 162.125.81.1, 2620:100:6031:1::a27d:5101\n",
            "Connecting to www.dropbox.com (www.dropbox.com)|162.125.81.1|:443... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: /s/dl/aiy87847kusb7ju/tok.valid.title.txt [following]\n",
            "--2019-12-02 17:15:41--  https://www.dropbox.com/s/dl/aiy87847kusb7ju/tok.valid.title.txt\n",
            "Reusing existing connection to www.dropbox.com:443.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://uc327081136b3432f54e38b64e81.dl.dropboxusercontent.com/cd/0/get/AtcZwqoDyFwWPxOJwCH2kjrIyjlj7oShrWQ0nu77gmN21EPkmQVakqJTIS5MQ31R2Z4Dwk9u-xzaN2v9A1OyywrKO81SqyvqFgQ7U0swaYopFq3qyxADN8XZXr1LWtv3u4U/file?dl=1# [following]\n",
            "--2019-12-02 17:15:41--  https://uc327081136b3432f54e38b64e81.dl.dropboxusercontent.com/cd/0/get/AtcZwqoDyFwWPxOJwCH2kjrIyjlj7oShrWQ0nu77gmN21EPkmQVakqJTIS5MQ31R2Z4Dwk9u-xzaN2v9A1OyywrKO81SqyvqFgQ7U0swaYopFq3qyxADN8XZXr1LWtv3u4U/file?dl=1\n",
            "Resolving uc327081136b3432f54e38b64e81.dl.dropboxusercontent.com (uc327081136b3432f54e38b64e81.dl.dropboxusercontent.com)... 162.125.81.6, 2620:100:6031:6::a27d:5106\n",
            "Connecting to uc327081136b3432f54e38b64e81.dl.dropboxusercontent.com (uc327081136b3432f54e38b64e81.dl.dropboxusercontent.com)|162.125.81.6|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 711754 (695K) [application/binary]\n",
            "Saving to: ‘tok.valid.title.txt’\n",
            "\n",
            "tok.valid.title.txt 100%[===================>] 695.07K   389KB/s    in 1.8s    \n",
            "\n",
            "2019-12-02 17:15:44 (389 KB/s) - ‘tok.valid.title.txt’ saved [711754/711754]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VZGjRsj3TdyM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "with open('tok.valid.abstract.txt','rb') as f:\n",
        "    body_data = f.read().decode(\"utf-8\").split('\\n')\n",
        "    \n",
        "with open('tok.valid.title.txt','rb') as f:\n",
        "    target_data = f.read().decode(\"utf-8\").split('\\n')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-tZMq4lCWA5x",
        "colab_type": "text"
      },
      "source": [
        "Create vocabulary using keras tokenizer "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QRFn7JpiUc6l",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "vocab_size = 20000\n",
        "tokenizer = tf.keras.preprocessing.text.Tokenizer(num_words=vocab_size, oov_token='<UNK>')\n",
        "tokenizer.fit_on_texts(body_data)\n",
        "tokenizer.index_word[vocab_size] = '<s>'\n",
        "tokenizer.index_word[vocab_size+1] = '<\\s>'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "21DoU69J9k75",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Vocab(object):\n",
        "  def __init__(self, max_size):\n",
        "    self._word_to_id = {}\n",
        "    self._id_to_word = {}\n",
        "    self._count = 0 # keeps track of total number of words in the Vocab\n",
        "    self._word_to_id['<PAD>'] = self._count\n",
        "    self._id_to_word[self._count] = \"<PAD>\"\n",
        "    self._count += 1\n",
        "    for _, word in tokenizer.index_word.items():\n",
        "      if word in self._word_to_id:\n",
        "        raise Exception('Duplicated word in vocabulary file: %s' % w)\n",
        "      self._word_to_id[word] = self._count\n",
        "      self._id_to_word[self._count] = word\n",
        "      self._count += 1\n",
        "      if max_size != 0 and self._count >= max_size:\n",
        "        break\n",
        "    print(\"Finished constructing vocabulary of %i total words. Last word added: %s\" % (self._count, self._id_to_word[self._count-1]))\n",
        "  def word2id(self, word):\n",
        "    \"\"\"Returns the id (integer) of a word (string). Returns [UNK] id if word is OOV.\"\"\"\n",
        "    if word not in self._word_to_id:\n",
        "      return self._word_to_id['UNK']\n",
        "    return self._word_to_id[word]\n",
        "\n",
        "  def id2word(self, word_id):\n",
        "    \"\"\"Returns the word (string) corresponding to an id (integer).\"\"\"\n",
        "    if word_id not in self._id_to_word:\n",
        "      raise ValueError('Id not found in vocab: %d' % word_id)\n",
        "    return self._id_to_word[word_id]\n",
        "\n",
        "  def size(self):\n",
        "    \"\"\"Returns the total size of the vocabulary\"\"\"\n",
        "    return self._count"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fZ2u3Xt39mfF",
        "colab_type": "code",
        "outputId": "c630e162-9793-49d5-d686-5f798911f858",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "vocab = Vocab(vocab_size+2)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Finished constructing vocabulary of 20002 total words. Last word added: <\\s>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j0dDtN5jmUlQ",
        "colab_type": "text"
      },
      "source": [
        "Convert text to numeric indicies"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lKe9LKo7WZnN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "body_seqs=tokenizer.texts_to_sequences(body_data)\n",
        "target_seqs=tokenizer.texts_to_sequences(target_data)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "61FYCoAHL1Qq",
        "colab_type": "text"
      },
      "source": [
        "Add start and end tokens to all sequences. Start token index = vocab.size and end token index = vocab.size+1"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iAQioivgL0d7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "body_seqs = [[vocab_size]+seq+[vocab_size+1] for seq in body_seqs]\n",
        "target_seqs = [[vocab_size]+seq+[vocab_size+1] for seq in target_seqs]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7dI4drSSmeH9",
        "colab_type": "text"
      },
      "source": [
        "Pad all sequences with zeros up to the maximum sequence length"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lgghEtRqXDCR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "max_len_body = max([len(seq) for seq in body_seqs])\n",
        "max_len_target = max([len(seq) for seq in target_seqs])\n",
        "body_seqs=tf.keras.preprocessing.sequence.pad_sequences(body_seqs, maxlen=max_len_body, padding=\"post\")\n",
        "target_seqs=tf.keras.preprocessing.sequence.pad_sequences(target_seqs, maxlen=max_len_target, padding=\"post\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3p67XH2G9-L0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_vector(word):\n",
        "    if word in word2vec_model:\n",
        "        return word2vec_model[word]\n",
        "    else:\n",
        "        return None"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6pS8fBOZ-Dxn",
        "colab_type": "code",
        "outputId": "51388c2d-de56-4a85-b4a9-19864706ec20",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        }
      },
      "source": [
        "import gzip\n",
        "#if not os.path.exists('GoogleNews-vectors-negative300.bin.gz'):\n",
        "!wget -P /root/input/ -c \"https://s3.amazonaws.com/dl4j-distribution/GoogleNews-vectors-negative300.bin.gz\""
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2019-12-02 17:15:51--  https://s3.amazonaws.com/dl4j-distribution/GoogleNews-vectors-negative300.bin.gz\n",
            "Resolving s3.amazonaws.com (s3.amazonaws.com)... 52.216.88.221\n",
            "Connecting to s3.amazonaws.com (s3.amazonaws.com)|52.216.88.221|:443... connected.\n",
            "HTTP request sent, awaiting response... 416 Requested Range Not Satisfiable\n",
            "\n",
            "    The file is already fully retrieved; nothing to do.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SQXuQc9x-HI5",
        "colab_type": "code",
        "outputId": "fea7e0fb-9f80-4d67-bef0-d2ad1b7938ec",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        }
      },
      "source": [
        "import gensim\n",
        "\n",
        "word2vec_model = gensim.models.KeyedVectors.load_word2vec_format('/root/input/GoogleNews-vectors-negative300.bin.gz', binary=True)  \n",
        "embedding_matrix = np.zeros((vocab_size+2, 300))\n",
        "for word, i in tokenizer.word_index.items():\n",
        "    if i > vocab_size+2:\n",
        "      break\n",
        "    temp = get_vector(word)\n",
        "    if temp is not None:\n",
        "        embedding_matrix[i] = temp\n",
        "file.download(embedding_matrix)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/smart_open/smart_open_lib.py:402: UserWarning: This function is deprecated, use smart_open.open instead. See the migration notes for details: https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function\n",
            "  'See the migration notes for details: %s' % _MIGRATION_NOTES_URL\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-13-b7b3a504a40e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mtemp\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0membedding_matrix\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtemp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0mfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdownload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membedding_matrix\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'file' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d0iPCVdBmn0b",
        "colab_type": "text"
      },
      "source": [
        "Create a dataset from which batches of a certain size can be extracted"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4rl-aGPvYU4n",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "buffer_size = len(body_seqs)\n",
        "batch_size = 16\n",
        "train_dataset = tf.data.Dataset.from_tensor_slices((body_seqs,target_seqs))\n",
        "train_dataset = train_dataset.shuffle(buffer_size).batch(batch_size, drop_remainder=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cg2rZviFl_MA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Encoder(tf.keras.Model):\n",
        "    def __init__(self, vocab_size, embedding_dim, hidden_units):\n",
        "        super().__init__()\n",
        "        \n",
        "        self.hidden_units = hidden_units\n",
        "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim, weights=[embedding_matrix])\n",
        "        self.bi_gru = tf.keras.layers.Bidirectional(tf.keras.layers.GRU(\n",
        "            hidden_units,\n",
        "            return_sequences=True,\n",
        "            return_state=True,\n",
        "            recurrent_initializer='glorot_uniform',\n",
        "        ))\n",
        "        \n",
        "    def call(self, encoder_input,encoder_states):\n",
        "        # inputs: encoder_input = (batch_size, seq_length)\n",
        "        #         encoder_states = list[(batch_size, hidden_units),(batch_size, hidden_units)]\n",
        "        \n",
        "        # embedding look-up layer\n",
        "        encoder_emb = self.embedding(encoder_input) # (batch_size,seq_length,embedding_dim)\n",
        "        \n",
        "        # encoder_output = (batch_size,seq_length,hidden_units)\n",
        "        # encoder_states = (batch_size,hidden_units)\n",
        "        encoder_output, state_fwd, state_back = self.bi_gru(encoder_emb,initial_state=encoder_states)\n",
        "        encoder_states = [state_fwd,state_back]\n",
        "\n",
        "        return encoder_output, encoder_states"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IsySdqmepD5q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class BahdanauAttention(tf.keras.Model):\n",
        "    def __init__(self, hidden_units,is_coverage=False):\n",
        "        super().__init__()\n",
        "        \n",
        "        self.Wh = tf.keras.layers.Dense(hidden_units) # weight matrix for encoder hidden state\n",
        "        self.Ws = tf.keras.layers.Dense(hidden_units) # weight matrix for decoder state\n",
        "        self.wc = tf.keras.layers.Dense(1) # weight vector for coverage\n",
        "        self.V = tf.keras.layers.Dense(1)\n",
        "        self.coverage = is_coverage\n",
        "        if self.coverage is False:\n",
        "            self.wc.trainable = False\n",
        "        \n",
        "    def call(self, decoder_state, encoder_output,coverage_vector):\n",
        "        # inputs: decoder_state = (batch_size, hidden_units)\n",
        "        #         encoder_output = (batch_size, seq_length, hidden_units)\n",
        "        #         coverage_vector = (batch_size, seq_length)\n",
        "\n",
        "        # expand dimension of decoder state and coverage vector to allow addition\n",
        "        decoder_state = tf.expand_dims(decoder_state, 1) # (batch_size, 1, hidden_units)\n",
        "        coverage_vector = tf.expand_dims(coverage_vector, 1) # (batch_size, 1, seq_length)\n",
        "\n",
        "        # calculate attention scores\n",
        "        # score = (batch_size, length, 1)\n",
        "        score = self.V(tf.nn.tanh(\n",
        "                        self.Wh(encoder_output) +  # (batch_size, length, hidden_units) -> (batch_size, length, attention_units)\n",
        "                        self.Ws(decoder_state) +  # (batch_size, 1, hidden_units) -> (batch_size, 1, attention_units)\n",
        "                        self.wc(coverage_vector) # (batch_size, 1, seq_length) -> (batch_size, 1, 1)\n",
        "                        )) \n",
        "        \n",
        "        attention_weights = tf.nn.softmax(score, axis=1) # (batch_size, seq_length, 1)\n",
        "        # only update coverage vector if coverage is enabled\n",
        "        coverage_vector = tf.squeeze(coverage_vector,1) # (batch_size, seq_length)\n",
        "        if self.coverage is True:\n",
        "          coverage_vector+=tf.squeeze(attention_weights) \n",
        "\n",
        "        context_vector = attention_weights * encoder_output # (batch_size, seq_length, hidden_units)\n",
        "        context_vector = tf.reduce_sum(context_vector, axis=1) # (batch_size, hidden_units)\n",
        "\n",
        "        return context_vector, attention_weights, coverage_vector"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DBONK9JFpGFj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Decoder(tf.keras.Model):\n",
        "    def __init__(self, vocab_size, embedding_dim, hidden_units):\n",
        "        super().__init__()\n",
        "        \n",
        "        self.hidden_units = hidden_units\n",
        "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim,  weights=[embedding_matrix])\n",
        "        self.gru = tf.keras.layers.GRU(\n",
        "            hidden_units,\n",
        "            return_sequences=True,\n",
        "            return_state=True,\n",
        "            recurrent_initializer='glorot_uniform',\n",
        "        )\n",
        "        self.W1 = tf.keras.layers.Dense(hidden_units)\n",
        "        self.W2 = tf.keras.layers.Dense(vocab_size)\n",
        "        # Pointer Generator\n",
        "        # wh = tf.keras.layers.Dense(1)\n",
        "        # ws = tf.keras.layers.Dense(1)\n",
        "        # wx = tf.keras.layers.Dense(1)\n",
        "\n",
        "        \n",
        "    def call(self, decoder_input, decoder_state, encoder_output,context_vector):\n",
        "        # inputs: decoder_input = (batch_size, 1)\n",
        "        #         decoder_state = (batch_size, hidden_units)\n",
        "        #         encoder_output = (batch_size,seq_length, hidden_units)\n",
        "        #         coverage_vector = (batch_size,seq_length)\n",
        "\n",
        "        # embedding look-up layer\n",
        "        decoder_emb = self.embedding(decoder_input) # (batch_size, seq_length, hidden_units)\n",
        "\n",
        "        # decoder_output = (batch_size,seq_length,hidden_units)\n",
        "        # decoder_state = (batch_size,hidden_units)\n",
        "        decoder_output , decoder_state = self.gru(decoder_emb,initial_state=decoder_state)\n",
        "\n",
        "        # concatenate context vector and decoder state \n",
        "        concat_vector = tf.concat([context_vector,decoder_state], axis=-1)\n",
        "        # reshape to 1d array\n",
        "        concat_vector = tf.reshape(concat_vector, (-1, concat_vector.shape[1]))\n",
        "        # create vocabulary distribution\n",
        "        p_vocab = tf.nn.log_softmax(self.W2(self.W1(concat_vector)))\n",
        "\n",
        "        # calculate p_gen\n",
        "        #p_gen = tf.nn.sigmoid(self.wh(context_vector)+self.ws(decoder_state)+self.wx(decoder_input))\n",
        "          \n",
        "        return p_vocab, decoder_state"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9udcRhiTdREd",
        "colab_type": "text"
      },
      "source": [
        "Initialize encoder and decoder"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cIdpMfncpK5q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "embedding_dim = 300\n",
        "hidden_units = 128\n",
        "encoder = Encoder(tokenizer.num_words+2, embedding_dim, hidden_units) # +2 on due to start and end tokens\n",
        "attention = BahdanauAttention(hidden_units,is_coverage=True)\n",
        "decoder = Decoder(tokenizer.num_words+2, embedding_dim, hidden_units)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OlPleBGNkdLQ",
        "colab_type": "text"
      },
      "source": [
        "Run one batch through the model in order to initialize model parameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hsM9QUZpkaMu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "encoder_input, decoder_target = next(iter(train_dataset))\n",
        "encoder_init_states = [tf.zeros((batch_size, encoder.hidden_units)) for i in range(2)]\n",
        "encoder_output, encoder_states = encoder(encoder_input,encoder_init_states)\n",
        "decoder_state = encoder_states[0] \n",
        "coverage_vector = tf.zeros((16,encoder_input.shape[1]))\n",
        "decoder_input_t = decoder_target[:,0]\n",
        "context_vector, attention_weights, coverage_vector = attention(decoder_state, encoder_output,coverage_vector)\n",
        "p_vocab,decoder_state = decoder(tf.expand_dims(decoder_input_t,1),decoder_state,encoder_output,context_vector)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C2kcLbzOhrlt",
        "colab_type": "text"
      },
      "source": [
        "Load pretrained weights if needed"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z2180O-yPUEh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# model_save_path = \"\"\n",
        "# encoder_save_name = \"encoder_weights30.h5\"\n",
        "# decoder_save_name = \"decoder_weights30.h5\"\n",
        "# attention_save_name = \"attention_30epochs.h5\"\n",
        "# encoder.load_weights(os.path.join(model_save_path,encoder_save_name))\n",
        "# decoder.load_weight(os.path.join(model_save_path,decoder_save_name))\n",
        "# attention.load_weights(os.path.join(model_save_path,attention_save_name))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pC567knspwSz",
        "colab_type": "text"
      },
      "source": [
        "Define optimizer and loss function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sVNa8U62pvUO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "optimizer = tf.keras.optimizers.Adam()\n",
        "\n",
        "def nll_loss(p_vocab,target):\n",
        "    # apply a mask such that pad zeros do not affect the loss\n",
        "    mask = tf.math.logical_not(tf.math.equal(target, 0))\n",
        "    loss = -p_vocab\n",
        "    mask = tf.cast(mask, dtype=loss.dtype)\n",
        "    loss *= mask  \n",
        "    return loss\n",
        "\n",
        "def coverage_loss(attention_weights,coverage_vector,target):\n",
        "    mask = tf.math.logical_not(tf.math.equal(target, 0))\n",
        "    coverage_vector = tf.expand_dims(coverage_vector,axis=2)\n",
        "    ct_min = tf.reduce_min(tf.concat([attention_weights,coverage_vector],axis=2),axis=2)\n",
        "    cov_loss = tf.reduce_sum(ct_min,axis=1)\n",
        "    mask = tf.cast(mask, dtype=cov_loss.dtype)\n",
        "    cov_loss *= mask\n",
        "    return cov_loss"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ctb2Zl8RuxnA",
        "colab_type": "text"
      },
      "source": [
        "Define a function for performing one training step (one batch)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ee51wXkDkfcO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "@tf.function\n",
        "def train_step(encoder_input, decoder_target):\n",
        "    \"\"\"Function which performs one training step (batch)\"\"\"\n",
        "    loss = tf.zeros(batch_size)\n",
        "    lambda_cov = 1\n",
        "    with tf.GradientTape() as tape:\n",
        "        # run body_sequence input through encoder\n",
        "        encoder_init_states = [tf.zeros((batch_size, encoder.hidden_units)) for i in range(2)]\n",
        "        encoder_output, encoder_states = encoder(encoder_input,encoder_init_states)\n",
        "        # initialize decoder with encoder forward state\n",
        "        decoder_state = encoder_states[0] # !!!interpolate between forward and backward instead!!!\n",
        "        coverage_vector = tf.zeros((16,encoder_input.shape[1]))\n",
        "        # loop over each word in target sequence\n",
        "        for t in range(decoder_target.shape[1]-1):\n",
        "            # run decoder input through decoder and generate vocabulary distribution\n",
        "            decoder_input_t = decoder_target[:,t]\n",
        "            decoder_target_t = decoder_target[:,t+1]\n",
        "            # get attention scores\n",
        "            context_vector, attention_weights, coverage_vector = attention(decoder_state, encoder_output,coverage_vector)\n",
        "            # get vocabulary distribution for each batch at time t\n",
        "            p_vocab,decoder_state = decoder(tf.expand_dims(decoder_input_t,1),decoder_state,encoder_output,context_vector)\n",
        "            # for each batch get the probability of the target word at time t+1\n",
        "            p_vocab_list = []\n",
        "            for i in range(len(decoder_target_t)):\n",
        "                p_vocab_list.append(p_vocab[i,decoder_target_t[i]])\n",
        "            p_vocab_target = tf.stack(p_vocab_list)\n",
        "            # calculate the loss at each time step t and add to current loss\n",
        "            loss += nll_loss(p_vocab_target,decoder_target_t) + lambda_cov*coverage_loss(attention_weights,coverage_vector,decoder_target_t)\n",
        "\n",
        "        # get the non-padded length of each sequence in the batch\n",
        "        seq_len_mask = tf.cast(tf.math.logical_not(tf.math.equal(decoder_target, 0)),tf.float32)\n",
        "        batch_seq_len = tf.reduce_sum(seq_len_mask,axis=1)\n",
        "\n",
        "        # get batch loss by dividing the loss of each batch by the target sequence length and mean\n",
        "        batch_loss = tf.reduce_mean(loss/batch_seq_len)\n",
        "\n",
        "    # update trainable variables\n",
        "    variables = encoder.trainable_variables + decoder.trainable_variables\n",
        "    gradients = tape.gradient(batch_loss, variables)\n",
        "    optimizer.apply_gradients(zip(gradients, variables))\n",
        "    \n",
        "    return batch_loss"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XH_-kaqadXXo",
        "colab_type": "text"
      },
      "source": [
        "Training loop"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T56FA7YkX2A_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tqdm import tqdm_notebook as tqdm\n",
        "\n",
        "epochs = 10\n",
        "\n",
        "epoch_loss = tf.keras.metrics.Mean()\n",
        "with tqdm(total=epochs) as epoch_progress:\n",
        "    for epoch in range(epochs):\n",
        "        epoch_loss.reset_states()\n",
        "\n",
        "        with tqdm(total=len(body_seqs) // batch_size) as batch_progress:\n",
        "            for batch, (encoder_input, decoder_target) in enumerate(train_dataset):\n",
        "                batch_loss = train_step(encoder_input, decoder_target)\n",
        "                epoch_loss(batch_loss)\n",
        "                \n",
        "                if (batch % 10) == 0:\n",
        "                    batch_progress.set_description(f'Epoch {epoch + 1}')\n",
        "                    batch_progress.set_postfix(Batch=batch, Loss=batch_loss.numpy())\n",
        "                batch_progress.update()\n",
        "        \n",
        "        epoch_progress.set_description(f'Epoch {epoch + 1}')\n",
        "        epoch_progress.set_postfix(Loss=epoch_loss.result().numpy())\n",
        "        epoch_progress.update()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0ZCMatONESEz",
        "colab_type": "text"
      },
      "source": [
        "Evaluate model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-zh_Vi0NEi2A",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def greedy_search(encoder_input,max_sum_len = 20):\n",
        "    \"\"\"Function which returns a summary by always picking the highest probability option conditioned on the previous word\"\"\"\n",
        "    # run body_sequence through encoder\n",
        "    encoder_init_states = [tf.zeros((1, encoder.hidden_units)) for i in range(2)]\n",
        "    encoder_output, encoder_states = encoder(encoder_input,encoder_init_states)\n",
        "    # initialize decoder with encoder forward state\n",
        "    decoder_state = encoder_states[0]\n",
        "\n",
        "    decoder_input_t = tf.ones(1)*tokenizer.num_words # initialize with start token\n",
        "    summary = [tokenizer.num_words]\n",
        "    coverage_vector = tf.zeros((1,encoder_input.shape[1]))\n",
        "    while decoder_input_t[0].numpy()!=(tokenizer.num_words+1) and len(summary)<max_sum_len: # as long as decoder input is different from end token continue\n",
        "        context_vector, attention_weights, coverage_vector = attention(decoder_state, encoder_output,coverage_vector)\n",
        "        p_vocab, decoder_state = decoder(tf.expand_dims(decoder_input_t,1),decoder_state,encoder_output,context_vector)\n",
        "        decoder_input_t = tf.argmax(p_vocab,axis=1) \n",
        "        decoder_word_idx = int(decoder_input_t[0].numpy())\n",
        "        summary.append(decoder_word_idx)\n",
        "    return summary"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-yWZKItF6bzg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def best_k_candidates(prev,k,encoder_output):\n",
        "    # use decoder to generate probability over vocabulary for the next token\n",
        "    decoder_input_t = tf.expand_dims(tf.expand_dims(prev[0][-1],0),1)\n",
        "    context_vector, attention_weights, coverage_vector = attention(prev[2][0], encoder_output,prev[2][1])\n",
        "    p_vocab, decoder_state = decoder(decoder_input_t, prev[2][0],encoder_output,context_vector)    \n",
        "    # sort the hypothesis by probability\n",
        "    idx = tf.argsort(p_vocab, direction='DESCENDING')  # sorted indices\n",
        "    ranks = tf.argsort(idx, direction='ASCENDING')  # ranks\n",
        "    filter_k = ranks < k # return True in the position with rank 1, 2, ..., k\n",
        "    # just for convenience, change to numpy\n",
        "    size = vocab_size+2 \n",
        "    p_vocab = p_vocab.numpy().reshape(size,)\n",
        "    filter_k = filter_k.numpy().reshape(size,)\n",
        "    # get the best hypothesis\n",
        "    best_k_candidates = [ [prev[0]+[x],p_vocab[x]+prev[1],[decoder_state,coverage_vector]] for x in range(size) if filter_k[x] ]\n",
        "    return best_k_candidates"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1w7QDpxS7KdO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def best_k_of_k2(best_k,k,completed,encoder_output):\n",
        "    results = []\n",
        "    # for each previous hypothesis, find k best hypothesis induced from it. THis will result in k^2 hypothesis\n",
        "    for hypo in best_k:\n",
        "      results = results + best_k_candidates(hypo, k, encoder_output)\n",
        "    # sort the list and extract k best hypothesis\n",
        "    results = sorted(results,key = lambda x: x[1],reverse= True)[0:k]\n",
        "    # if there is a completed hypothesis (end token generated), transfer it to completed set and decrease the beam size\n",
        "    for result in results:  \n",
        "      if result[0][-1] == vocab_size+1:\n",
        "        k-=1\n",
        "        results.remove(result)\n",
        "        completed += [result]\n",
        "    return results,k,completed"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sVihBTAE7ekD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def beam_search(encoder_input, max_sum_len = 20, beam_width = 5):\n",
        "      # run body_sequence input through encoder\n",
        "      encoder_init_states = [tf.zeros((1, encoder.hidden_units)) for i in range(2)]\n",
        "      encoder_output, encoder_states = encoder(encoder_input,encoder_init_states)\n",
        "\n",
        "      # initialize decoder with encoder forward states\n",
        "      decoder_states = encoder_states[0]\n",
        "      coverage_vector = tf.zeros((1,encoder_input.shape[1]))\n",
        "      # initialize the hypothesis: [sequence <s>, log probability 0, decoder states]\n",
        "      prev = [[vocab_size],0,[decoder_states,coverage_vector]]\n",
        "      # get the beam size and create a list to store completed hypothesis \n",
        "      k = beam_width\n",
        "      completed = []\n",
        "      # get k best first token\n",
        "      best_k = best_k_candidates(prev,k,encoder_output)\n",
        "\n",
        "      # use beam search for max_sum_len (maximum length) steps\n",
        "      for i in range(max_sum_len):\n",
        "        # get k best hypothesis when adding a new token\n",
        "        best_k,k,completed = best_k_of_k2(best_k,k,completed,encoder_output)\n",
        "        # stop when there are enough completed hypothesis\n",
        "        if len(completed) == k:\n",
        "          break \n",
        "      \n",
        "      # when there are no completed hypothesis, take 5 last hypothesis as the final candidates\n",
        "      if len(completed) == 0:\n",
        "        completed = best_k\n",
        "      # normalized the hypothesis probability by the length of hypothesis\n",
        "      for hypo in completed:\n",
        "        hypo[1]/=len(hypo[0])\n",
        "        hypo = [hypo[1],hypo[2]]   \n",
        "      # sort the hypothesis by normalized probability and choose the best one       \n",
        "      best_k = sorted(completed,key=lambda x: x[1],reverse=True)[0][0]\n",
        "      return best_k"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lj9oGQo9w3GB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "encoder_save_name = \"encoder_30epochs.h5\"\n",
        "decoder_save_name = \"decoder_30epochs.h5\"\n",
        "attention_save_name = \"attention_30epochs.h5\"\n",
        "encoder.save_weights(encoder_save_name)\n",
        "decoder.save_weights(decoder_save_name)\n",
        "attention.save_weights(attention_save_name)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ktHV__vixcE9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# files.download(attention_save_name)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V7XqpsXBu_Bn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "encoder_input, decoder_target = next(iter(train_dataset))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ShWnw-PzEai2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "encoder_input_sum = tf.expand_dims(encoder_input[0,:],0)\n",
        "summary_greedy = greedy_search(encoder_input_sum)\n",
        "summary_beam = beam_search(encoder_input_sum)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QEjMdLyQEdUc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "target_summary = [d for d in decoder_target.numpy()[0] if d!=0]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CJmkncscEfr1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(\"Generated by greedy search:\"+\" \".join([tokenizer.index_word[idx] for idx in summary_greedy]))\n",
        "print(\"Generated by beam search:\"+\" \".join([tokenizer.index_word[idx] for idx in summary_beam]))\n",
        "print(\"Target:\"+\" \".join([tokenizer.index_word[idx] for idx in target_summary]))"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}