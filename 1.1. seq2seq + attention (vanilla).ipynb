{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "LSTM John CS492_PointerGenerator.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "z6YcmNYRS2XN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "try:\n",
        "    %tensorflow_version 2.x\n",
        "except Exception:   \n",
        "    pass\n",
        "\n",
        "import tensorflow as tf\n",
        "import os\n",
        "import numpy as np"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8XFPMTnOV7rV",
        "colab_type": "text"
      },
      "source": [
        "Load the text files"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7iSTmwbxS7cJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "if not os.path.exists('tok.valid.abstract.txt'):\n",
        "  !wget -O 'tok.valid.abstract.txt' 'https://www.dropbox.com/s/0c6e9yf8yhf9a75/tok.valid.abstract.txt?dl=1'\n",
        "if not os.path.exists('tok.valid.body.txt'):\n",
        "  !wget -O 'tok.valid.title.txt' 'https://www.dropbox.com/s/aiy87847kusb7ju/tok.valid.title.txt?dl=1'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VZGjRsj3TdyM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "with open('tok.valid.abstract.txt','rb') as f:\n",
        "    body_data = f.read().decode(\"utf-8\").split('\\n')\n",
        "    \n",
        "with open('tok.valid.title.txt','rb') as f:\n",
        "    target_data = f.read().decode(\"utf-8\").split('\\n')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-tZMq4lCWA5x",
        "colab_type": "text"
      },
      "source": [
        "Create vocabulary using keras tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QRFn7JpiUc6l",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tokenizer = tf.keras.preprocessing.text.Tokenizer(oov_token='<UNK>')\n",
        "tokenizer.fit_on_texts(body_data)\n",
        "vocab = dict(map(reversed, tokenizer.word_index.items()))\n",
        "vocab[len(vocab)+1] = '<s>'\n",
        "vocab[len(vocab)+1] = '</s>'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j0dDtN5jmUlQ",
        "colab_type": "text"
      },
      "source": [
        "Convert text to numeric indicies"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lKe9LKo7WZnN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "body_seqs=tokenizer.texts_to_sequences(body_data)\n",
        "target_seqs=tokenizer.texts_to_sequences(target_data)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "61FYCoAHL1Qq",
        "colab_type": "text"
      },
      "source": [
        "Add start and end tokens to all sequences. Start token index = vocab.size and end token index = vocab.size+1"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iAQioivgL0d7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "body_seqs = [[len(vocab)-1]+seq+[len(vocab)] for seq in body_seqs]\n",
        "target_seqs = [[len(vocab)-1]+seq+[len(vocab)] for seq in target_seqs]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7dI4drSSmeH9",
        "colab_type": "text"
      },
      "source": [
        "Pad all sequences with zeros up to the maximum sequence length"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lgghEtRqXDCR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "max_len_body = max([len(seq) for seq in body_seqs])\n",
        "max_len_target = max([len(seq) for seq in target_seqs])\n",
        "body_seqs=tf.keras.preprocessing.sequence.pad_sequences(body_seqs, maxlen=max_len_body, padding=\"post\")\n",
        "target_seqs=tf.keras.preprocessing.sequence.pad_sequences(target_seqs, maxlen=max_len_target, padding=\"post\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KC_b71mynh_3",
        "colab_type": "text"
      },
      "source": [
        "Reserve a held-out set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FAL5kupqezVY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "m=len(body_seqs)//10\n",
        "body_seqs_val = body_seqs[:m]\n",
        "body_seqs = body_seqs[m:]\n",
        "target_seqs_val = target_seqs[:m]\n",
        "target_seqs = target_seqs[m:] \n",
        "print(len(body_seqs_val),len(target_seqs_val))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d0iPCVdBmn0b",
        "colab_type": "text"
      },
      "source": [
        "Create a dataset from which batches of a certain size can be extracted"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4rl-aGPvYU4n",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "buffer_size = len(body_seqs)\n",
        "batch_size = 16\n",
        "dataset_tensor = tf.data.Dataset.from_tensor_slices((body_seqs,target_seqs))\n",
        "validation_tensor = tf.data.Dataset.from_tensor_slices((body_seqs_val,target_seqs_val))\n",
        "batch_size_val = 1\n",
        "dataset_tensor = dataset_tensor.shuffle(buffer_size).batch(batch_size, drop_remainder=True)\n",
        "validation_tensor = validation_tensor.shuffle(buffer_size).batch(1, drop_remainder=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cg2rZviFl_MA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Encoder(tf.keras.Model):\n",
        "    def __init__(self, vocab_size, embedding_dim, hidden_units):\n",
        "        super().__init__()\n",
        "        \n",
        "        self.hidden_units = hidden_units\n",
        "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
        "        self.bi_lstm = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(\n",
        "            self.hidden_units,\n",
        "            return_state=True,\n",
        "            return_sequences=True\n",
        "        ))\n",
        "        \n",
        "    def call(self, encoder_input,encoder_state):\n",
        "        # inputs: encoder_input = (batch_size, seq_length)\n",
        "        #         encoder_state = (batch_size, hidden_units)\n",
        "        \n",
        "        # embedding look-up layer\n",
        "        encoder_emb = self.embedding(encoder_input) # (batch_size,seq_length,embedding_dim)\n",
        "        \n",
        "        # encoder_output = (batch_size,seq_length,hidden_units)\n",
        "        # states = (batch_size,hidden_units)\n",
        "        encoder_output, state_h_fwd, state_c_fwd, state_h_back, state_c_back = self.bi_lstm(encoder_emb,initial_state=encoder_state)\n",
        "        encoder_states = [state_h_fwd,state_c_fwd]\n",
        "\n",
        "        return encoder_output, encoder_states"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IsySdqmepD5q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class BahdanauAttention(tf.keras.layers.Layer):\n",
        "    def __init__(self, hidden_units):\n",
        "        super().__init__()\n",
        "        \n",
        "        self.Wh = tf.keras.layers.Dense(hidden_units) # weight matrix for encoder hidden state\n",
        "        self.Ws = tf.keras.layers.Dense(hidden_units) # weight matrix for decoder state\n",
        "        self.V = tf.keras.layers.Dense(1)\n",
        "        \n",
        "    def call(self, decoder_state, encoder_output):\n",
        "        # inputs: decoder_state = (batch_size, hidden_units)\n",
        "        #         encoder_output = (batch_size, seq_length, hidden_units)\n",
        "        \n",
        "        # expand dimension of decoder state to allow addition\n",
        "        decoder_state = tf.expand_dims(decoder_state, 1) # decoder_state = (batch_size, 1, hidden_units)\n",
        "        \n",
        "        # calculate attention scores\n",
        "        score = self.V(tf.nn.tanh(self.Wh(encoder_output) # (batch_size, seq_length, hidden_units) -> (batch_size, seq_length, attention_units)\n",
        "                      +self.Ws(decoder_state))) # (batch_size, 1, hidden_units) -> (batch_size, 1, attention_units)\n",
        "        \n",
        "        # attention_weights = (batch_size, length, 1)\n",
        "        attention_weights = tf.nn.softmax(score, axis=1)\n",
        "\n",
        "        # context_vector = (batch_size, length, hidden_units)\n",
        "        context_vector = attention_weights * encoder_output\n",
        "        # context_vector = (batch_size, hidden_units)\n",
        "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
        "\n",
        "        return context_vector, attention_weights"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DBONK9JFpGFj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Decoder(tf.keras.Model):\n",
        "    def __init__(self, vocab_size, embedding_dim, hidden_units):\n",
        "        super().__init__()\n",
        "        \n",
        "        self.batch_size = batch_size\n",
        "        self.hidden_units = hidden_units\n",
        "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
        "        self.lstm = tf.keras.layers.LSTM(\n",
        "            self.hidden_units,\n",
        "            return_sequences=True,\n",
        "            return_state=True,\n",
        "        )\n",
        "        self.attention = BahdanauAttention(self.hidden_units)\n",
        "        self.W1 = tf.keras.layers.Dense(hidden_units)\n",
        "        self.W2 = tf.keras.layers.Dense(vocab_size)\n",
        "      \n",
        "    def call(self, decoder_input, decoder_init_state, encoder_output,mode=\"Train\"):\n",
        "        # inputs: decoder_input = (batch_size, 1)\n",
        "        #         decoder_state = (batch_size, hidden_units)\n",
        "        #         encoder_output = (batch_size, seq_length, hidden_units)\n",
        "        if mode == \"Train\":\n",
        "            # embedding look-up layer\n",
        "            decoder_input = self.embedding(decoder_input) # (batch_size, seq_length, embeding_dim)\n",
        "\n",
        "            # decoder_output = (batch_size,seq_length,hidden_units)\n",
        "            # states = (batch_size,hidden_units)\n",
        "            decoder_output, state_h, state_c = self.lstm(decoder_input,initial_state=decoder_init_state)\n",
        "            decoder_state = [state_h,state_c]\n",
        "\n",
        "            # get context vector and attention weights\n",
        "            context_vector, attention_weights = self.attention(state_h, encoder_output)\n",
        "\n",
        "            # concatenate context vector and decoder state \n",
        "            concat_vector = tf.concat([context_vector,state_h], axis=-1)\n",
        "            # reshape to 1d array\n",
        "            concat_vector = tf.reshape(concat_vector, (-1, concat_vector.shape[1]))\n",
        "            # create vocabulary distribution\n",
        "            p_vocab = tf.nn.log_softmax(self.W2(self.W1(concat_vector)))\n",
        "            return p_vocab \n",
        "\n",
        "        if mode == \"Test\":\n",
        "            # inputs: decoder_input = (batch_size, 1)\n",
        "            #         decoder_state = (batch_size, hidden_units)\n",
        "            #         encoder_output = (batch_size, seq_length, hidden_units)\n",
        "            \n",
        "            # embedding look-up layer\n",
        "            decoder_input = self.embedding(decoder_input) # (batch_size, seq_length, hidden_units)\n",
        "\n",
        "            # decoder_output = (batch_size,seq_length,hidden_units)\n",
        "            # states = (batch_size,hidden_units)\n",
        "            decoder_output, state_h, state_c = self.lstm(decoder_input,initial_state=decoder_init_state)\n",
        "            decoder_states = [state_h,state_c]\n",
        "\n",
        "            # get context vector and attention weights\n",
        "            context_vector, attention_weights = self.attention(state_h, encoder_output)\n",
        "\n",
        "            # concatenate context vector and decoder state \n",
        "            concat_vector = tf.concat([context_vector,state_h], axis=-1)\n",
        "            # reshape to 1d array\n",
        "            concat_vector = tf.reshape(concat_vector, (-1, concat_vector.shape[1]))\n",
        "            # create vocabulary distribution\n",
        "            p_vocab = tf.nn.log_softmax(self.W2(self.W1(concat_vector)))\n",
        "            return p_vocab, decoder_states"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9udcRhiTdREd",
        "colab_type": "text"
      },
      "source": [
        "Initialize encoder and decoder"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cIdpMfncpK5q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "embedding_dim = 128\n",
        "hidden_units = 256\n",
        "encoder = Encoder(len(vocab)+1, embedding_dim, hidden_units) # +2 on due to start and end tokens\n",
        "decoder = Decoder(len(vocab)+1, embedding_dim, hidden_units)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pC567knspwSz",
        "colab_type": "text"
      },
      "source": [
        "Define optimizer and loss function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sVNa8U62pvUO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "optimizer = tf.keras.optimizers.Adam()\n",
        "\n",
        "def masked_nll(p_vocab,target):\n",
        "    # apply a mask such that pad zeros do not affect the loss\n",
        "    mask = tf.math.logical_not(tf.math.equal(target, 0))\n",
        "    loss = -p_vocab\n",
        "    mask = tf.cast(mask, dtype=loss.dtype)\n",
        "    loss *= mask\n",
        "    return loss"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ctb2Zl8RuxnA",
        "colab_type": "text"
      },
      "source": [
        "Define a function for performing one training step (one batch)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ee51wXkDkfcO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "@tf.function\n",
        "def train_step(encoder_input, decoder_target):\n",
        "    \"\"\"Function which performs one training step (batch)\"\"\"\n",
        "    loss = tf.zeros(batch_size)\n",
        "    with tf.GradientTape() as tape:\n",
        "        # run body_sequence input through encoder\n",
        "        encoder_init_states = [tf.zeros((batch_size, encoder.hidden_units)) for i in range(4)]\n",
        "        encoder_output, encoder_states = encoder(encoder_input,encoder_init_states)\n",
        "        # initialize decoder with encoder forward state\n",
        "        decoder_state = encoder_states\n",
        "        \n",
        "        # loop over each word in target sequence\n",
        "        for t in range(decoder_target.shape[1]-1):\n",
        "            # run decoder input through decoder and generate vocabulary distribution\n",
        "            decoder_input_t = decoder_target[:,t]\n",
        "            decoder_target_t = decoder_target[:,t+1]\n",
        "            # get vocabulary distribution for each batch at time t\n",
        "            p_vocab = decoder(tf.expand_dims(decoder_input_t,1),decoder_state,encoder_output)\n",
        "            # for each batch get the probability of the target word at time t+1\n",
        "            p_vocab_list = []\n",
        "            for i in range(len(decoder_target_t)):\n",
        "                p_vocab_list.append(p_vocab[i,decoder_target_t[i]])\n",
        "            p_vocab_target = tf.stack(p_vocab_list)\n",
        "            # calculate the loss at each time step t and add to current loss\n",
        "            loss += masked_nll(p_vocab_target,decoder_target_t)\n",
        "            \n",
        "        # get the non-padded length of each sequence in the batch\n",
        "        seq_len_mask = tf.cast(tf.math.logical_not(tf.math.equal(decoder_target, 0)),tf.float32)\n",
        "        batch_seq_len = tf.reduce_sum(seq_len_mask,axis=1)\n",
        "\n",
        "    # get batch loss by dividing the loss of each batch by the target sequence length and mean\n",
        "    batch_loss = tf.reduce_mean(loss/batch_seq_len)\n",
        "    \n",
        "    # update trainable variables\n",
        "    variables = encoder.trainable_variables + decoder.trainable_variables\n",
        "    gradients = tape.gradient(loss, variables)\n",
        "    optimizer.apply_gradients(zip(gradients, variables))\n",
        "    \n",
        "    return batch_loss"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XH_-kaqadXXo",
        "colab_type": "text"
      },
      "source": [
        "Training loop"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T56FA7YkX2A_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tqdm import tqdm_notebook as tqdm\n",
        "\n",
        "epochs = 5\n",
        "\n",
        "epoch_loss = tf.keras.metrics.Mean()\n",
        "with tqdm(total=epochs) as epoch_progress:\n",
        "    for epoch in range(epochs):\n",
        "        epoch_loss.reset_states()\n",
        "\n",
        "        with tqdm(total=len(body_seqs) // batch_size) as batch_progress:\n",
        "            for batch, (encoder_input, decoder_target) in enumerate(dataset_tensor):\n",
        "                batch_loss = train_step(encoder_input, decoder_target)\n",
        "                epoch_loss(batch_loss)\n",
        "                \n",
        "                if (batch % 10) == 0:\n",
        "                    batch_progress.set_description(f'Epoch {epoch + 1}')\n",
        "                    batch_progress.set_postfix(Batch=batch, Loss=batch_loss.numpy())\n",
        "                batch_progress.update()\n",
        "        \n",
        "        epoch_progress.set_description(f'Epoch {epoch + 1}')\n",
        "        epoch_progress.set_postfix(Loss=epoch_loss.result())\n",
        "        epoch_progress.update()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E6SExItm7ZEU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def best_k_candidates(prev,k,encoder_output):\n",
        "    decoder_target_t = tf.expand_dims(tf.expand_dims(prev[0][-1],0),1)\n",
        "    p_vocab, decoder_states = decoder(decoder_target_t,prev[2],encoder_output,mode=\"Test\")\n",
        "    size = len(vocab)+1\n",
        "    idx = tf.argsort(p_vocab, direction='DESCENDING')  # sorted indices\n",
        "    ranks = tf.argsort(idx, direction='ASCENDING')  # ranks\n",
        "    filter_k = ranks < k\n",
        "    p_vocab = p_vocab.numpy().reshape(size,)\n",
        "    filter_k = filter_k.numpy().reshape(size,)\n",
        "    best_k_candidates = [ [prev[0]+[x],p_vocab[x]+prev[1],decoder_states] for x in range(size) if filter_k[x] ]\n",
        "    return best_k_candidates\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "gn0g7_IwZsxT",
        "colab": {}
      },
      "source": [
        "def best_k_of_k2(best_k,k,completed,encoder_output):\n",
        "  results = []\n",
        "  for hypo in best_k:\n",
        "    results = results + best_k_candidates(hypo, k, encoder_output)\n",
        "  results = sorted(results,key = lambda x: x[1],reverse= True)[0:k]\n",
        "\n",
        "  for result in results:  \n",
        "    if result[0][-1] == len(vocab):\n",
        "      k-=1\n",
        "      results.remove(result)\n",
        "      completed += [result]\n",
        "  return results,k,completed"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8-5M4KeONH1m",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def val_step(encoder_input, beam_width):\n",
        "      # run body_sequence input through encoder\n",
        "      encoder_init_states = [tf.zeros((1, hidden_units)) for i in range(4)]\n",
        "      encoder_output, encoder_states = encoder(encoder_input,encoder_init_states)\n",
        "      # initialize decoder with last encoder states\n",
        "      decoder_states = encoder_states\n",
        "      prev = [[len(vocab)-1],0,decoder_states]\n",
        "      # decoder_target_t = tf.expand_dims(tf.expand_dims(prev[0],0),1)\n",
        "      # p_vocab = val_decoder(decoder_target_t,decoder_states,encoder_output)\n",
        "      # log_p_vocab = map(lambda x: log(x),p_vocab.numpy())\n",
        "      k = beam_width\n",
        "      completed = []\n",
        "      best_k = best_k_candidates(prev,k,encoder_output)\n",
        "      for i in range(10):\n",
        "        best_k,k,completed = best_k_of_k2(best_k,k,completed,encoder_output)\n",
        "        if len(completed) == k:\n",
        "          break \n",
        "      if len(completed) == 0:\n",
        "        completed = best_k\n",
        "      for hypo in completed:\n",
        "        hypo[1]/=len(hypo[0])\n",
        "        hypo = [hypo[1],hypo[2]]         \n",
        "      best_k = sorted(completed,key=lambda x: x[1],reverse=True)\n",
        "      return best_k[0][0]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qbijbllzj25Z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "k = 5\n",
        "from tqdm import tqdm_notebook as tqdm\n",
        "with tqdm(total=len(body_seqs_val)) as batch_progress:\n",
        "    for i,(encoder_input, x) in enumerate(validation_tensor):\n",
        "        if i>5:\n",
        "          break\n",
        "        print(i,\" : \",body_data[i],\"\\n\",target_data[i])\n",
        "        pred = val_step(encoder_input,5)\n",
        "        pred = list(map(lambda x: vocab[x], pred))\n",
        "        print(str(pred))"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}